<!DOCTYPE html>
<html>
  <head>
    <title>Teach a Neural Network how to Math</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Open+Sans);
      @import url(https://fonts.googleapis.com/css?family=Roboto:300);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono);

      html {
        color: #222;
        font-size: 1em;
        line-height: 1.4;
      }
      body {
        font-family: 'Roboto', sans-serif;
      }
      h1, h2, h3 {
        line-height: 1.04;
        letter-spacing: -.028em;
      }
      .small {
        font-size: 50%;
        display: block;
      }
      .med {
        font-size: 75%;
        display: block;
      }
      .med li {
        padding-bottom: 12px;
      }
      .remark-slide-content {
        background-size: cover;
        line-height: 1.58;
        font-size: 32px;
        padding: 1em 3em 1em 3em;
        word-break: break-word;
        word-wrap: break-word;
        text-rendering: optimizeLegibility;
        -webkit-font-smoothing: antialiased;
        color: rgba(0,0,0,.8);
      }
      .remark-slide-content img {
        max-width: 90%;
        max-height: 90%;
      }
      .remark-code, .remark-inline-code {
        font-family: 'Ubuntu Mono';
        text-align: left;
        font-size: 17px;
        line-height: .5;
      }
      a:visited {
        color: blue;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

name: title
class: center, middle

# Teach a Neural Network<br />How to Math

???
Hi everybody and welcome to my presentation "Teach a nn how to math". This is
not very Python-specific and would actually require some background knowledge,
but is fun nonetheless and I promise to make it as simple as possible.

---

name: max
class: center, middle

# Max Schumacher

Machine Learning and Full Stack Engineer<br/>
based in Germany

???
About me first, my name is Max and I'm a just passing by here from Germany,
where I work as a Machine Learning and Full Stack Engineer guy for various
startups.

---

Name: dl
class: center, middle

# Deep Learning

.center[![Cat](https://static.pexels.com/photos/137049/pexels-photo-137049.jpeg)]

???
At the base of this talk really is this amazing revolution in AI we are
witnessing right now, and that I'm sure you've all at least heard about.

Originally, machines used to be good at doing stuff like Math, while humans
were good at pattern recognition, such as recognizing cats in pictures. That
used to be close to impossible for machines to do, but now thanks to Deep
Learning, this is starting to shift.

---

Name: translation
class: center, middle

# Machine Translation

.center[![Machine Translation](https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg)]

.small[Source: https://esciencegroup.com/2016/03/04/fun-with-recurrent-neural-nets-one-more-dive-into-cntk-and-tensorflow/]

???
One of the fields where we've seen the most amazing results is Machine
Translation. You might have noticed that Google Translate received a significant
boost in quality in the last year, and this is thanks to exactly the technique
we will be implementing today.

At the base of most of Deep Learning are Neural Networks. It's out of the scope
to explain what they are, but just think of them as black boxes for now that,
given a lot training examples, can learn to approximate very very complex
functions.

In tasks like translation we're working with sequences of data, e.g.
sequences of words or characters, and want to output a different sequence.
For this we use a special kind of NN called RNN. The most popular ones are
called LSTM and have a powerful memory algorithm.

---

Name: equations
class: center, middle

# Math Equations

451 + 834 = 1285<br />
19 + 977 = 996<br />
762 + 639 = 1401

???
Now our problem today won't be the translation of natural language, but rather
learning how to solve very simple math equations. Our model will look at a
equation string character by character and then output the result character by
character.

This might seem trivial, but remember that this NN has never seen anything like
equations before, and basically has to learn what all this stuff means and how
to add numbers up from scratch.

Also, if this still does not knock you off your feet: Remember that the exact
same algorithm is used to power Google Translate. And other Deep Learning
models follow similar patterns, so if you understand this you are really only
a few steps away from doing real magic.

---

Name: equations2
class: center

# Creating the Math Equations

```python
import itertools
import random

def generate_equations(shuffle=True, max_count=None):
    """
    Generates all possible math equations given the global configuration.
    If max_count is given, returns that many at most. If shuffle is True,
    the equation will be generated in random order.
    """
    # Generate all possible unique sets of numbers
    number_permutations = itertools.permutations(
        range(MIN_NUMBER, MAX_NUMBER + 1), 2
    )

    # Shuffle if required. The downside is we need to convert to list first
    if shuffle:
        number_permutations = list(number_permutations)
        random.shuffle(number_permutations)

    # If a max_count is given, use itertools to only look at that many items
    if max_count is not None:
        number_permutations = itertools.islice(number_permutations, max_count)

    # Build an equation string for each and yield to caller
    for x, y in number_permutations:
        yield '{} + {}'.format(x, y)
```

???
So in order that you can guys can go home today and feel like you actually
looked at some Python code, let's have a look at my equation generator.

We use the awesome itertools standard library to create all possible
permutations of two numbers out of our target range. For machine learning,
it's always a good idea to shuffle you data, but in that case, we have to
load the whole thing into memory, sadly.

Finally for each permutation we yield one additions string.

---

Name: results
class: center

# Getting the results

```python
>>> eval('1 + 2')
3
```

???
Now to train and validate the neural network, we also need the result for each
formula in our dataset. But this is easy to get, since each formula is real
Python and can be evaluated to the result of the addition.

---

Name: results
class: center

# Here begins the Magic

.center[![Machine Translation](https://1.bp.blogspot.com/-qBcFdZvxYJ0/WLBZmmTfe_I/AAAAAAAAAYQ/uqLYPAXb1-09ZB1xAwqVkqn2B8q1gl4TwCLcB/s1600/hacking-01-matrix.jpg)]

.small[Source: http://www.mrread.net/2017/02/12-langkah-supaya-mahir-di-bidang.html]

???
So, this is about as much code as can be easily explained without any prior
knowledge. I'll skip over the rest out of time reasons, but trust me, it is
much easier than it might seem. The amazing thing about Neural Networks and
Deep Learning is how simple yet powerful the underlying ideas are.

If you really wanted to learn how to do this, you could after reading up for
a few hours. Especially if use a high level framework like Keras, things are
very straightforward.

---

Name: encoding
class: center

# Encoding the Data:<br />One Hot Vectors

.center[![Machine Translation](https://cdn-images-1.medium.com/max/1600/1*pgWSPyximAFHqZtUkiLeKg.png)]

.small[Source: https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b]

???
One thing we have to do before we feed in our data into the model, is to
encode it in a way the neural network can work with. It doesn't really care
about how we encode it, because it's just going to work with that encoding.
However, we need a way to later decode the result again to yield real strings.

NNs expect their inputs to be vectors or matrixes of a specific shape, so one
way to encode single characters is as so-called one-hot vectors.

This means that a vector is all zeros except for one. This one value signifies
which character is represented.

---

Name: encoding
class: center

# Encoding the Data:<br />Example

```python
import numpy as np

CHARS = [str(n) for n in range(10)] + ['+', ' ']
CHAR_TO_INDEX = {i: c for c, i in enumerate(CHARS)}
INDEX_TO_CHAR = {c: i for c, i in enumerate(CHARS)}

def char_to_one_hot(char):
    index = CHAR_TO_INDEX[char]
    length = len(CHARS)

    vector = np.zeros(length)
    vector[index] = 1.

    return vector
```

???
So to give you a basic example how this encoding could be built in Numpy.

First we think of all the possible characters we could expect in our problem
space, i.e. all numbers and plus and space.

Then we build a dictionary to map them to indexes and back.

Now to convert a character to a one_hot vector, all we need to do is create
vector with 0s for all possible characters, then set the one at the right index
to 1.


---

Name: model
class: center

# Building the Model

```python
from keras.models import Sequential
from keras.layers import LSTM, RepeatVector, Dense, Dropout, Activation
from keras.layers.wrappers import TimeDistributed

def build_model():
    """
    Builds and returns the model based on the global config.
    """
    input_shape = (9, len(CHARS))

    model = Sequential()
    model.add(LSTM(256, input_shape=input_shape))
    model.add(RepeatVector(4))
    model.add(LSTM(256, return_sequences=True))
    model.add(TimeDistributed(Dense(len(CHARS))))
    model.add(Activation('softmax'))

    model.compile(
        loss='categorical_crossentropy',
        optimizer='adam',
        metrics=['accuracy'],
    )

    return model
```

???
We're building the model in the awesome Keras framework.

The basic shape of the data:

Input: MAX_EQ_L x N_CHARS, aka one one-hot vector per character in the input.

The structure of the model is a bit too complex to go into here, but I want
to emphasize that it's not important in Neural Networks to get the perfect
structure. We can kind of wing it and maybe tune it later on, but generally
the algorithms are very robust to structure. It's actually quite fun to
experiment with them and see how the perfomance changes.

Finally, we're getting MAX_R_L x N_CHARS outputs, so basically the result
string.


---

Name: training
class: center

# Training the Model

```python
model.fit(
    x_train, y_train,
    epochs=100,
    validation_data=(x_test, y_test),
)
```

```python
Train on 46781 samples, validate on 15594 samples

Epoch 1/100
  158s - loss: 1.4898 - acc: 0.4248 - val_loss: 1.2601 - val_acc: 0.4894
Epoch 2/100
  158s - loss: 1.1510 - acc: 0.5499 - val_loss: 1.0708 - val_acc: 0.5831
Epoch 3/100
  171s - loss: 1.0617 - acc: 0.5857 - val_loss: 1.0117 - val_acc: 0.6111
Epoch 4/100
  154s - loss: 1.0193 - acc: 0.6053 - val_loss: 0.9814 - val_acc: 0.6283
Epoch 5/100
  154s - loss: 0.8838 - acc: 0.6576 - val_loss: 0.6767 - val_acc: 0.7513

...
```

???
After we built the model, all we need to do is train it.

A quick note on the dataset: It is very important to retain some of the
available data as a "test" or "validation" set. The model is not allowed to
train on this data, but is periodically checkd how well it fares on.

Also, as a note, I only took about 5% of the all possible equations for
training, so it's really not just learning by heart but has to grog the idea.

So basically, this makes sure the model does not simply learn the training
examples by heart, but learns to generalize to other, unseen cases.

Running this takes a bit, but quite quickly we can see the validation accuracy
coming close to 1, meaning it fares really well.


---

Name: plot
class: center

# Training the Model

.center[![Plot showing the metrics over training time](https://cpury.github.io//assets/images/math_figure_5.png)]

???
Now if we plot the test accuracy numbers over time, we see how it converges
against 1 over time. This means we're doing a really great job!

Finally, I've reached an accuracy of 0.999 after about 60 training epochs.


---

Name: testing
class: center

# Testing the Output

Before the training:

```python
Example predictions:
696 + 277 = +++66   (expected:  973)
395 + 425 = 11113   (expected:  820)
829 + 687 = 8888+   (expected: 1516)
782 + 879 =         (expected: 1661)
354 + 849 =     7   (expected: 1203)
```

--

After the training:

```python
Examples:
875 + 470 = 1345   (expected: 1345)
257 + 514 =  771   (expected:  771)
984 + 787 = 1771   (expected: 1771)
248 + 129 =  377   (expected:  377)
113 +  36 =  149   (expected:  149)
```

???
To test the NN, we put in an equation string, let it flow through the network,
and see what it gives us as the most likely output string.

Before we started the training, the network had completely random weights
and was thus outputting gibberish on any query.

Now let's check what happens after the training: It seems to give us perfect
results!


---

Name: mistakes
class: center

# Analyzing the Mistakes

.center[![Plot showing the metrics over training time](https://cpury.github.io//assets/images/math_figure_4.png)]

???
Since our equations only have two factors of variation (the two numbers),
we can easily plot them on a 2D plane using matplotlib. Green dots signify
correct results, while red dots signify mistakes.

We can see that most mistakes happen in the smaller bounds, e.g. where numbers
are below 100 or even 10.

This could be analyzed and improved on, but that is out of the scope here.

SHOW EXAMPLE MODEL


---

Name: end

# Further Reading

.med[
* These slides: http://cpury.github.io/misc/math/
* More detail: http://cpury.github.io/learning-math/
* Even more detail: https://github.com/cpury/lstm-math
* Intro to ML: https://www.coursera.org/learn/machine-learning
* Intro to DL: https://www.coursera.org/learn/neural-networks-deep-learning
* Keras framework: https://keras.io/
]

???
Alright folks that's it. I hope you enjoyed this little experiment. You can
find these slides online under this address. Also, this was based on a blog post
I wrote a few weeks ago. It goes into more details and contains all of the code
neccessary to run it yourself. There's also a github repo that has even more
features and tweaks.

If this made you curious about Deep Learning or Machine Learning in general,
I can recommend these two Coursera courses to get you started.

Or if you want to jump straight to the practical parts, check out Keras, an
amazing Python framework that abstracts away a lot of the complicated parts
so you can focus on building models. There are tonnes of tutorials for Keras.



    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        highlightStyle: 'tomorrow-night',
      });
    </script>
  </body>
</html>
