---
layout: post
title: "AI Against Humanity"
date:   2019-06-23 13:21:19
description: "How an experiment turned into a AI-driven browser game"
categories: machine-learning
permalink: ai-against-humanity/
tags: [machine-learning, nlp]
disqus: true
image: /assets/images/aiah.png
---

**Do you like Cards Against Humanity or messed-up humor?** Try out my new browser game [AI Against Humanity](https://www.aiagainsthumanity.app/). All the cards you see were generated by a neural network. You're also competing against another neural network! But beware: It contains some **extremely strong and offensive language** üòá

<!-- TODO Big NSFW warning! -->

<div class="cta-wrapper">
    <a href="https://www.aiagainsthumanity.app/" class="cta" target="_blank">Check out the result üöÄ</a>
</div>

In this blog post, I want to detail the process of how a **little experiment** with a language model turned into a **complete browser game** including an **AI opponent**. I hope it can serve as a **case study of how to quickly build and ship ML / web ideas**.

### TL;DR

- The cards were generated by a **GPT-2 language model** fine-tuned on Cards Against Humanity text
- The frontend to the game is built with **jQuery** and **TensorFlow.js** and 100% **statically hosted**
- I use a simple **AWS Lambda** function to keep track of user interactions
- The AI opponent uses PCA-reduced **BERT-Embeddings** and a small neural network to make decisions
- It was trained on **aggregated interaction data** of users playing the game in the early phase

### The Idea

Five years ago, I wrote a [blog post](https://cpury.github.io/learning-holiness/) about letting language models write new Bible texts. The basic idea is to have a neural network reading a text (i.e. from the Bible) and then making it generate new text based on that. Since then, a lot has happened in the world of language models. The two major changes are:

1. **Better model architectures** for reading and writing language. 5 years ago, we used RNNs. Now, we use **Transformers**.
2. **Pre-training** on a large corpus. Similar to computer vision, we can get vast advantages by first pretraining the model on a larger dataset and then fine-tuning it on whatever we need.

Many people have written articles explaining the two, and I will not repeat here what others can do better than me. To understand the Transformer architecture, check out [Jay Alammar‚Äôs The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/). To understand the pre-trained model we use here, check out the [official announcement for GPT-2](https://openai.com/blog/better-language-models/), the model I used and current state of the art in terms of language generation.

When OpenAI released their (reduced) GPT-2 model, I felt compelled to repeat my Bible experiments from back then. Can we create even better Bible quotes? What about other kinds of text? I fine-tuned the pre-trained model on the King James Bible, all of Trump‚Äôs tweets and some Wikipedia articles. The results were really impressive! See the [appendix](#appendix-a-bible-quotes) for examples.

This is when I saw a post on Reddit about [creating Cards Against Humanity cards using a Char-LSTM](https://www.reddit.com/r/MachineLearning/comments/bpvif8/p_generating_cards_against_humanity_cards/). Having the dirty humor of a 14-year-old and thus being a natural fan of CAH, I loved the idea! So, apologizing to u/ablacklama for taking his idea, I fine-tuned a GPT-2 model on CAH cards and checked out the results. They were hilarious! Here are some early examples:

> ‚ÄúFarting Your Way To The Center Of The Earth.‚Äù

> ‚ÄúSexy grandmothers swinging by.‚Äù

> ‚ÄúLeaning against an oak tree and taking live horses for granted.‚Äù

> ‚ÄúA very sexy mummy.‚Äù

I was hooked. From there on, the roadmap became clear: Instead of simply letting people read these new cards in a list and forget about them the next minute, why not let them interact with them? I could build a web app where people can play a basic version of the game, all on fake cards. Then, I could collect which card combos users prefer and train a neural network on this to understand common humor. Finally, I could use this trained model to power an AI opponent!

So I went to work.



### Creating the cards

Downloading and fine-tuning GPT-2 is extremely easy, fast and free thanks to OpenAI and Google's Colab.

If you want to try it out yourself, simply start a new notebook on Colab, install the awesome [gpt-2-simple package](https://github.com/minimaxir/gpt-2-simple) and get going. It even comes with a [great example notebook](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce)

I did this same thing on a dataset of Cards Against Humanity questions and answers. You can obtain all sets of cards as JSON from [here](https://crhallberg.com/cah/). I noticed quickly that you need to separate the questions and answers data and fine-tune two separate models. Creating good questions is much harder than random funny answer cards.


### Creating the frontend prototype

The goal from the beginning was to run the whole game inside a static website and let JS do all of the heavy lifting. For the prototype, I went with good old [jQuery](https://jquery.com/), though if the project keeps growing I might switch to [Vue](https://vuejs.org/) at some point. For the CSS framework, I wanted to try something new and so went with [Bulma](https://bulma.io/), which is pretty nice.

Basically, I keep the pre-generated cards along with a unique ID in text files. The frontend then loads and shuffles these and shows questions and answers to the user. No biggie so far, right?


### Deploying the frontend and logging user interactions

I‚Äôm a big fan of [AWS](https://aws.amazon.com) [S3](https://aws.amazon.com/s3/) and [Lambda](https://aws.amazon.com/lambda/). You can go really far without paying more than a dollar a month, and you don‚Äôt have to worry about dev ops at all. So the frontend and all data assets are deployed to S3 using [s3_website](https://github.com/laurilehmijoki/s3_website).

It gets trickier when you want to log user interactions, i.e. which cards combo users think funny and which they don‚Äôt. Basically, I need an API that simply logs whatever I send to it and lets me export these logs later. Traditionally, this would require a running server, hosting costs and actually worrying about dev ops. I spent some time trying to figure this out and finally arrived at this super simple hack:

A tiny **Lambda function** that simply logs everything it gets as an input to std-out. I can then access all the logs in CloudWatch and download them for further processing. This works for the moment and costs me very little. That said, it‚Äôs a bit hacky and if you do have a better idea, please don‚Äôt hesitate to comment below üôÇ


### Training and integrating the AI opponent

After posting this first prototype to [r/ml](https://www.reddit.com/r/MachineLearning/comments/bvwvoo/p_ai_against_humanity_play_cards_against_humanity/) and asking my friends to play it for a few days, I got a few tens of thousands of interactions to work with. There are all kinds of interactions I keep track of, e.g. given a question card and four answer candidates, which answer did the user choose?

Based on these data points, we can aggregate the choices into a mean probability for each question-answer-pair. With this, a model can learn the probability of playing answer A given question Q. Then we simply take the answer with the highest probability in each case.

So far so good, but remember we want everything to work without a server. This model, too, has to work in the browser and be fast enough to provide a smooth experience on any phone or laptop.

Thankfully, we have TensorFlow.js, the awesome JS-library that lets you train and use neural networks in the browser. If you need a starter on this, check out my [blog post](https://cpury.github.io/learning-where-you-are-looking-at/) from last year where I explain in great detail how to detect where a user is looking at on the screen, all in the browser (and also backend-less).

Great, so we have the means to export a TensorFlow model and run it in the browser. But if it‚Äôs supposed to run fast on any device, it needs to be tiny! And language and humor are hard problems‚Ä¶ So what now?

You have probably heard of [BERT](https://github.com/google-research/bert). We can use it to extract powerful sentence embeddings that contain a high-level representation of a given text in only 768 values. Wait, 768? Um, that‚Äôs still too much for our purposes, I‚Äôm afraid...

Here comes the next trick: Let‚Äôs use [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) to **reduce the dimensionality** of the vectors to something easier to wield. Actually, I found that just **10 dimensions** is enough for a baseline and more only help marginally.

So, for each pre-generated cards, I generated the BERT embedding, then fitted PCA on it and stored these 10 values for each card in another file. Our assumption is that they describe the topic and kind of card to a degree that we can then learn something from it with only a single hidden layer.

The **actual neural network** then takes **two of these embeddings** (one for the question, one for the answer), and **outputs a probability value**. There‚Äôs no magic here, a **single hidden layer** and some **input dropout** worked great for me.

To **deploy and use the trained model** in the game as an AI opponent, we load the model and the embeddings from S3 and ask it in each turn which of its (hidden) 4 answer cards it thinks most likely to be funny based on the question card. That‚Äôs it!


### The Result

I‚Äôm quite happy with the result. The game is fun, the AI player has some pretty sick moves, and the generated cards are often hilarious. I keep getting messages from users who said it got them in stitches. Depending on feedback I might extend this further. I‚Äôm also planning to release the frontend code on GitHub.

Let me know what you think and if you have any other ideas you‚Äôd like to see.


### Ideas for the future

- Multiplayer *
- SFW Version
- Let the model create new cards on-line *
- Multi-gap questions

\* These require a real server running in the back. Scandalous!


### Appendix A: Bible Quotes

Fine-tuned on the [King James Bible](https://github.com/ErikSchierboom/sentencegenerator/blob/master/samples/the-king-james-bible.txt).

##### Unconditioned:

> 10:31 And the LORD said unto Moses, This garment shall not be
> taken away from thee; neither shalt thou make any abode for thee among
> the children of Israel.
>
> 10:32 And the LORD said, Thus will I do unto you, that I will make you
> clean to do this, if thou wilt not take the garment, and put it
> on his head.
>
> 10:33 And the LORD said, Thou shalt not put it on the other his
> head: and he shall take it on his head.

See the numbering? That wasn't working with LSTMs.

> 7:19 He sat upon a couch with his feet upon the
> ground, and he lifted up his eyes unto heaven,
> and looked upon me.

Ohhhkay?

> And they had a king in Jerusalem, named David, who was a man of
> great stature, and full of honour, and had a crown of gold, and had
> the name of Solomon.

This sounds like an almost perfectly coherent quote, doesn't it? I checked, it does not appear in the original text. Also, David was Solomon's father, so the last part doesn't make sense. Otherwise, impressive!

##### Conditioned (query in *bold*):

> **And the LORD called Donald Trump a** prophet, and he is the son of Abraham, and the LORD has raised him up
> to the throne.
>
> 23:28 And they said unto him, Behold, thou hast been an example for all the
> fear of the LORD.

ü§Ø

> **And the LORD farted** and made a great noise, and the land was shaken.

Bahaha

> **And the LORD said unto him, my favorite color is** purple;
> it is not the same as blood.

Bold choice.

> **A man who lies with a man**, as one lying with a woman, is both holy and blameless.

Yay, go God!


### Appendix B: Trump Tweets

Because why not? Obtained from the [Trump Twitter Archive](http://www.trumptwitterarchive.com/). Preprocessed by replacing URLs and kicking out all tweets that have less than 100 retweets.

##### Unconditioned:

> I am in no way qualified to be the commander-in-chief of the GREAT UNITED STATES.

Dial back self-awareness by about 90%.

> I am a very happy man. I'm a great guy, I'm smart, and I'm a winner!

Is that you, Donald?

##### Conditioned (query in *bold*):

> **I want to sleep with** my daughter

Ouch!

> **I want to have sex with** all the girls at the Miss USA pageants.

Yikes, how can the model know him so well??

Finally, I asked Mr. Trump what the true meaning of [Covfefe](https://web.archive.org/web/20170531060414/https://twitter.com/realDonaldTrump/status/869766994899468288) is:

**The true meaning of covfefe is**

- that it's a great word!
- that I am not happy!
- "If you can't stand the truth, who can?"
- that I'm very glad I didn‚Äôt lose to Donald Trump in a Miss Universe pageant. I‚Äôve had enough of the phony media!


### Appendix C: Wikipedia Articles

Same as in [Andrej Karapathy's original post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), we use a [tiny
subset of Wikipedia](http://prize.hutter1.net/) to train. It has quite a bit of XML code in it, but the largest chunk should be pure Wikipedia text.

##### Unconditioned:

> Game of Thrones is in the process of acquiring its first female actor.
>
> British-Icelandic television series will feature Tyrion Lannister as the father who leads the army following the death of his beloved wife Catelyn Stark. The character will play Tyrion's firstborn son (and heir). And in a surprise twist, the show will also feature Cersei Lannister as Cersei's rival husband. HBO has hired former director of photography Jon Ronson to direct the series according to Variety.

Bahaha. This doesn't seem very Wikipedia-ish, maybe some artifacts from the untuned GPT-2?

> The term "physicist" was originally a way to describe individuals whose job was investigating the behavior of subatomic bodies. But in the context of physics (and especially physics-based medicine) one is usually better described as someone whose job was to create a world of physics.

ü§î

> A radical criminological approach to violent crime has been championed by the criminologist Donald A. Hackett, former Director of the National Institute of Justice. Products of his study include the phrase "The criminological approach": the study of crime as a system study from der trag√§r (state-inside affair) to serial attack.
>
> At just 2.2 years old, the study of crime came under the critical microscope by the French Deputy Prime Minister, Christophe January, who launched his institutional "suicide police" program; and in 1977 by President Ronald Reagan. The Gun Control Act of 1968 further legitimised the concept. Discrimination against the civilian population would eventually eventually lead to the justified fears of FBI agents, who no doubt regarded Carmans as dropped devourers of maternally deviant students into and beyond the university.

You can see the samples are not terribly coherent over longer paragraphs. But already much better than Karpathy's original samples.


##### Conditioned (query in *bold*):

> **Machine Learning is** one of the oldest and most advanced methods for training any machine, and one of the fastest methods of learning itself (http://www.informatik.net/Articles/machine_learning/). The main goals of machine learning are: 1. to predict the future by using knowledge obtained from the past; 2. to do automatic tasks to determine what things and systems are appropriate; 3. to create new things or new systems.

Note the fake URL.

> **Breathing is still the leading cause of** morbidity and mortality in Australia, but for the first six months (2004/05 to 2005/06) it has fallen from the top of the list in all countries surveyed.

Better stop now!

> **Nickelback are often viewed as** a cult classic, perhaps most notably in the form of the rock'n'roll soundtrack to the film "Candy Crush Saga."

I had no idea!

> **When the US Navy attacked South-West-Portugal in 2005,** the US government was very concerned that the Portuguese would respond in kind. Thereafter, the US sent a group of warships to the area to act as "battlers" against the Portuguese. The first group of ships headed to North-West-Portugal. On July 4, 2005, the US Navy attacked South-West-Portugal in a military exercise. By July 15, the US-backed naval blockade had been lifted. This had not been an easy decision for the US.

Fake news, I guess :)

> **Scientifically, brain farts are** thought to be about 2-6% of all human excretions.

I knew it!

> **Christiana Dakoto McCarthy II was** born in St Louis, Missouri on March 15, 1847. Her father, a tailor, and her mother, a professional nurse, had been separated at the age of 15, when her father, a prominent merchant, married a rich seamstress and raised her two sisters. Her mother and her sisters were all married to white businessmen. She worked as a maid, and became a skilled carpenter and handyman. In 1902 she married her first husband, a farmer, and had many children.

Wow! Fake biography of a fake person. The only weirdness is the year of her first marriage: In 1902, she should have been 55.

> **Drklav, pronounced** ÀÄ≈°…ælav, is the name of the last child (in the original Greek script) of Zeus. Zeus is the god of agriculture, and as the youngest father of the gods he is also the father of the race that produces the best grain.

> **Alexander the Great's darkest secret** was his obsession with immortality. In the fourth century B.C.E., in a time when he was on the verge of losing his head [or at the very least] his life, he built his kingdom at the base of Mount Olympus and raised the giant Olympus Athena as his divine bride for him. In the process he gained superhuman strength.
